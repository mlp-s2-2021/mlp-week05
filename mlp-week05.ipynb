{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-31c179b2-6190-4870-92d4-4fcc73b377e4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Machine Learning in Python - Workshop 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-171fa9ca-f8ad-4d3a-b258-66088209d191",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "\n",
    "## 1.1 Packages\n",
    "\n",
    "In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-63d053e6-e126-466a-893f-aa80e441791e",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-7df35566-2ff3-4a19-b587-2d6da8242502",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.2 Helper Functions\n",
    "\n",
    "Below are two helper functions we will be using in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-88bb7540-f836-4959-92fa-3cc44e24cd90",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(m):\n",
    "    \"\"\"Returns the model coefficients from a Scikit-learn model object as an array,\n",
    "    includes the intercept if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If pipeline, use the last step as the model\n",
    "    if (isinstance(m, sklearn.pipeline.Pipeline)):\n",
    "        m = m.steps[-1][1]\n",
    "    \n",
    "    \n",
    "    if m.intercept_ is None:\n",
    "        return m.coef_\n",
    "    \n",
    "    return np.concatenate([[m.intercept_], m.coef_])\n",
    "\n",
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the root mean squared error of a fitted model based on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should fit plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Fit plot\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y', y='resid', data=res).set_title(\"Residual plot\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model rmse = \" + str(round(rmse, 4)), fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-5e84c9ee-2493-4be5-bc23-0a10f8c08f24",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.3 Data\n",
    "\n",
    "The data for this week's workshop comes from the Elements of Statistical Learning textbook. The data originally come from a study by Stamey et al. (1989) in which they examined the relationship between the level of prostate-specific antigen (`psa`) and a number of clinical measures in men who were about to receive a prostatectomy. The variables are as follows,\n",
    "\n",
    "* `lpsa` - log of the level of prostate-specific antigen\n",
    "* `lcavol` - log cancer volume\n",
    "* `lweight` - log prostate weight\n",
    "* `age` - patient age\n",
    "* `lbph` - log of the amount of benign prostatic hyperplasia\n",
    "* `svi` - seminal vesicle invasion\n",
    "* `lcp` - log of capsular penetration\n",
    "* `gleason` - Gleason score\n",
    "* `pgg45` - percent of Gleason scores 4 or 5\n",
    "* `train` - test / train split used in ESL\n",
    "\n",
    "These data are available in `prostate.csv` which is provided with this worksheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-aa8c690a-f64b-4dfd-afd5-1633ab6dd90f",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "prostate = pd.read_csv('prostate.csv')\n",
    "prostate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-26db1217-df3a-4c1d-b5cc-11184893cc07",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "As before we will begin by constructing a pairs plot of our data and examining the relationships between our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-7fd82cfb-638e-4ea6-94ef-4633ae46da5b",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=prostate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-953ef5b5-3c6e-4f09-8218-0beaf21b9d7f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 1\n",
    "\n",
    "Are there any interesting patterns in these data? Specifically, your answer should address,\n",
    "* Do any of our variables appear to be categorical / ordinal rather than numeric?\n",
    "* Which variable appears likely to have the strongest relationship with `lpsa`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-90a94dbb-f1d9-4e1f-bfb5-a12b51e96619",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-77647a6e-0e16-4784-b565-8a90ea0f1b72",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 2\n",
    "\n",
    "Why do you think we are exploring the relationship between these variables and `lpsa` (log of psa) rather than just psa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-657d7729-d289-451f-ae2e-6070529aabfa",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-bdd5989d-0de2-46a8-9e7b-b21a4fa28e49",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.4 Validation Set\n",
    "\n",
    "For these data we have already been provided a column to indicate which values should be used for the training set and which for the validation set. This is encoded by the values in the `train` column - we can use these columns to separate our data and generate our training data: `X_train` and `y_train` as well as our test data `X_test` and `y_test`. As we will also need the complete data set we will also construct `X` and `y`, which contain all 97 observations but without the `train` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-3db93f72-4603-4bb5-88a6-7d6a8a2f410c",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Create train and validate data frames\n",
    "train = prostate.query(\"train == 'T'\").drop('train', axis=1)\n",
    "validate = prostate.query(\"train == 'F'\").drop('train', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-cac3133e-4287-4e1f-91c9-5eeae4798360",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = train.drop(['lpsa'], axis=1)\n",
    "y_train = train.lpsa\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-d11aed41-cc43-4b6f-a4b1-33bb6a6f45dd",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Validation data\n",
    "X_test = validate.drop('lpsa', axis=1)\n",
    "y_test = validate.lpsa\n",
    "\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-41a37fb0-a402-4677-9e81-dc57cbffcf9b",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Complete data\n",
    "X = prostate.drop(['lpsa','train'], axis=1)\n",
    "y = prostate.lpsa\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-c925b74c-07e0-4c12-b391-5abce0a18936",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.5 Baseline model\n",
    "\n",
    "Our first task is to fit a baseline model which we will be able to use as a point of comparison for our subsequent models. A good candidate for this is a simple linear regression model that includes all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-9bbd8663-92d4-47f4-9cea-670789a8bda7",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-a52b0205-9095-44af-b0a6-c89f65680e32",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We can extract the coefficients for the model, which correspond to the variables: `intercept`, `lcavol`, `lweight`, `age`, `lbph`, `svi`, `lcp`, `gleason`, and `pgg45` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-0e7ae13c-eb3b-49c6-97d0-64940f11b368",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "get_coefs(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-ca800932-1977-49dc-9540-5a9663d568fc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "These coefficients have the typical regression interpretation, e.g. for each unit increase in `lcavol` we expect `lpsa` to increase by 0.577 on average. These values are not of particular interest for us for this particular problem as we are more interested in the predictive properties of our model(s). To evaluate this we will use the `model_fit` helper function defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-8846a23e-b8ef-4636-bee1-5a6f6a7c642d",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "model_fit(lm, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-5ff15253-bc51-40ca-87c3-1aa76b2e53dd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Primarily we will use this function to obtain the rmse of our model using the validation data (`X_test` and `y_test`). Note that we fit the model using the training data (`X_train` and `y_train`). We have also included a fit ($y$ vs $\\hat{y}$) and resid ($y$ vs $y-\\hat{y}$) plot of these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-ecc760e5-7f5b-4bf9-9e2e-084475961770",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 3\n",
    "\n",
    "Based on these plots do you see anything in the fit or residual plot that is potentially concerning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-7c30e599-edcc-4d06-bb9a-2df37244e18d",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-2e7aa23d-149b-424e-9263-aa2e53bd0a70",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 4\n",
    "\n",
    "Would you expect the rmse of the model to be better or worse when using the training data (compared to the validation data)? Check your answer using the `model_fit` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-65ef1d17-b0ce-4518-98e7-45e79c8d9622",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00029-2e7be621-8bbc-4ec1-aa5e-09d862813de2",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-1c7ce039-e64b-4f77-80e3-6a913e63cee2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.6 Standardization\n",
    "\n",
    "In subsequent sections we will be exploring the use of the Ridge and Lasso regression models which both penalize larger values of $\\beta$. While not particularly bad, our baseline model had $\\beta$s that ranged from the smallest at 0.0095 to the largest at 0.737 which is about a 78x difference in magnitude. This difference can be made even worse if we were to change the units of one of our features, e.g. changing a measurement in kg to grams would change that coefficient by 1000 which has no effect on the fit of our linear regression model (predictions and other coefficients would be unchanged) but would have a meaningful impact on the estimates given by a Ridge or Lasso regression model, since that coefficient would now dominate the penalty term.\n",
    "\n",
    "To deal with this issue, the standard approach is to center and scale *all* features to a common scale before fitting one of these models. The typical scaling approach is to subtract the mean of each feature and then divide by its standard deviation - this results in all feature columns having a mean of 0 and a variance of 1. Additionally, the feature values can now be interpreted as the number of standard deviations each observation is away from that column's mean.\n",
    "\n",
    "Using sklearn we can perform this transformation using the `StandardScaler` transformer from the `preprocessing` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00031-c64e9fcc-04d8-423e-b85a-91f80ea3a23c",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-ffd79c11-93a9-4c66-9b2c-b06ae22b2789",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "S = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-993f0cfa-6e03-4956-967e-d09dc8c3c812",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Once fit, we can examine the values used for the scaling by checking the `mean_` and `var_` attributes of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-1f0e549d-e139-4e6f-b18b-2522e66a6b55",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "S.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-db0c4ddd-fb46-4c73-b357-68ad71572413",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "S.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-99d48b7e-eba7-4ff6-87e6-bf786ff6e0bb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Keep in mind, that the training, testing, and validation sets will not necessarily have the same feature column means and standard deviations - as such it is important that we choose a consistent set of values that are used for all of the data. In other words, be careful to not expect that `StandardScaler().fit_transform(X_train)` and  `StandardScaler().fit(X).transform(X_train)` will give the same answer. The best way to avoid this issue is to include the `StandardScaler` in a modeling pipeline for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-c5f47dcd-4538-4eaa-a22d-6b828a32763e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 5\n",
    "\n",
    "Explain why scaling `y`, `y_train`, or `y_test` is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-a066d79b-beaf-482e-a8af-76966b79f582",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00039-6341e9e8-d73d-428d-959d-554c0f25e3fa",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 6\n",
    "\n",
    "What are the units of the transformed features in `StandardScaler().fit_transform(X_train)`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-a073c803-71ac-4770-b2f8-a45e97e6f99e",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-81f55e86-42f9-40a3-92e3-d1ca1f1b9a5c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 1.7 Scaled Linear Regression\n",
    "\n",
    "Now that we have scaled the data, let us fit another simple linear regression model using these scaled features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00042-5abc0cb5-9170-4bba-84bf-2e9f2cab9f92",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "lm_scaled = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00043-029c5651-64d8-4b22-9db8-be9777e48c7f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Once fit we can extract the model coefficients and calculate the validation rmse,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00044-03f05b81-4a32-4f72-82e7-1a06446f20cc",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "get_coefs(lm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00045-13c7e03c-b91e-4d7a-bdc2-7d669c46ffb5",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "model_fit(lm_scaled, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-300e0172-21fb-46d5-9576-d1c41601a06f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 7\n",
    "\n",
    "Using this new model what has changed about our model results? Comment on both the model's coefficients as well as its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00047-070edf0a-2f45-4e9e-9fde-9b852f871f31",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00048-be7b44f2-4454-4119-9fb5-c412eea36a8d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "--- \n",
    "\n",
    "# 2. Ridge Regression\n",
    "\n",
    "Ridge regression is a natural extension to linear regression which introduces an $\\ell_2$ penalty on the coefficients to a standard least squares problem. Mathematically, we can express this as the following optimization problem,\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\; \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert^2 + \\alpha (\\boldsymbol{\\beta}^T\\boldsymbol{\\beta}) $$ \n",
    "\n",
    "The `Ridge` model is provided by the `linear_model` submodule and requires a single parameter `alpha` which determines the tuning parameter that adjusts the weight of the $\\ell_2$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00049-da30fac2-e467-4ceb-ac3c-a9b5860a6b3c",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-bd31bb07-c202-44a8-a9c2-ee1cb977cfd3",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "r = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=1)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "model_fit(r, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00051-e791465d-2a88-4318-b911-afac8f903afd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 8\n",
    "\n",
    "Adjust the value of `alpha` in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the rmse change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00052-350766bb-2e6a-4637-ade8-0ad358e8ee05",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00053-173046e4-465f-4b9e-b3f9-5c9f8e1f1211",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 9\n",
    "\n",
    "In Section 1.4 we mentioned the importance of scaling features before fitting a Ridge regression model. The code below fits the Ridge model to the untransformed training data - repeat Exercise 8 using these data. How does the model fit change as alpha changes? How does the rmse change? How does the models performance compare to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00054-b980a912-c590-473a-a03d-69ebb94715e9",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "r_wo_scale = make_pipeline(\n",
    "    Ridge(alpha=1)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "model_fit(r_wo_scale, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00055-5c2b0c77-8402-4495-94d9-56768f090eab",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00056-3009ee4b-e254-4eda-8a2b-4497ca22278b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 2.1 Ridge $\\beta$s as a function of $\\alpha$\n",
    " \n",
    "Finally, one of the useful ways of thinking about the behavior of Ridge regression models is to examine the relationship between our choice of $\\alpha$ and the resulting $\\beta$s relative to the results we would have obtained from the linear regression model. Since Ridge regression is equivalent to linear regression when $\\alpha=0$ we can see that as we increase the value of $\\alpha$ we are shrinking all of the $\\beta$s towards 0 asymptotically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00057-95699805-d802-4301-9bb2-36ad07773e79",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 3, num=200) # from 10^-2 to 10^3\n",
    "\n",
    "betas = [] # Store coefficients\n",
    "rmses = [] # Store validation rmses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge(alpha=a)\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    # We drop the intercept as it is not included in Ridge's l2 penalty and hence not shrunk\n",
    "    betas.append(get_coefs(m)[1:]) \n",
    "    rmses.append(model_fit(m, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00058-b770ed27-9ec3-4c93-951b-65a897b9747b",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(\n",
    "    data = betas,\n",
    "    columns = X.columns # Label columns w/ feature names\n",
    ").assign(\n",
    "    alpha = alphas,\n",
    "    rmse = rmses\n",
    ").melt(\n",
    "    id_vars = ('alpha', 'rmse')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00059-046ffb7e-ca82-439c-84fe-430c34f1968d",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='alpha', y='value', hue='variable', data=res).set_title(\"Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00060-e47dfb33-b612-4db0-82dc-8ef3b067fc78",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 10\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `lpsa`. *Hint* - think about what the degree of shrinkage towards 0 means in this context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-8fd722b8-ca55-46e0-b0c7-d3aedf89d8cd",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00062-a38b0ab3-d1d9-4327-83cd-10b67b185ee1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 2.2 Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00063-319c29a8-c160-43e5-9878-e5ba4b319299",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The $\\alpha$ in the Ridge regression model is another example of a hyperparameter, and just like the degree in a polynomial model we can use cross validation to attempt to identify the optimal value for our data. As with the polynomial models from last week we will start by using `GridSearchCV` to employ 5-fold cross validation to determine an optimal $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00064-25e9a738-01f4-42e8-9f69-225852928399",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(0, 15, num=151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00065-2a8b9bd0-d762-4d4f-a345-5090789d4eb3",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge()\n",
    "    ),\n",
    "    param_grid={'ridge__alpha': alphas},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00066-7842f93e-e458-4113-bbf3-2a40a08e6c9a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Note that we are passing `sklearn.model_selection.KFold(5, shuffle=True, random_state=1234)` to the `cv` argument rather than leaving it to its default. This is because, while not obvious, the prostate data is structured (sorted by `lpsa` value) and this way we are able to ensure that the folds are properly shuffled. Failing to do this causes *very* unreliable results from the cross validation process.\n",
    "\n",
    "Once fit, we can examine the results to determine what value of $\\alpha$ was chosen as well as examine the calculated mean of the rmses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00067-237e5808-85ee-46ac-9da4-8aaeb3ec486a",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00068-a0b2e4a1-3188-4ef1-9e0e-af3c143868e1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "To evaluate this model we can access the `best_estimator_` model object and use it to obtain an rmse for our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00069-fd52df20-b96c-40fa-b22c-7f07ade6775c",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "model_fit(gs.best_estimator_, X_test, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00070-a315db19-b166-466d-91d7-67f2ddb08452",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 11\n",
    "\n",
    "How does this model compare to the performance of our baseline model? Is it better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00071-eefb9e52-9764-4584-a134-b76cf9387399",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00072-c7e4b742-09f6-4727-9f5f-c9cf6c76123a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 12\n",
    "\n",
    "How do the model coefficient for this model compare to the base line model? *Hint* - be careful about which baseline model you compare with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00073-10239e8b-7bff-471e-896a-379b6c4ebaed",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00074-b3b01657-0aea-465f-a747-c06832b147ea",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "To further explore this choice of $\\alpha$, we can collect relevant data about the folds and their performance from the `cv_results_` attribute. In this case we are particularly interested in examining the `mean_test_score` and the `split#_test_score` keys since these are used to determine the optimal $\\alpha$.\n",
    "\n",
    "In the code below we extract these data into a data frame by selecting our columns of interest along with the $\\alpha$ values used (and transform negative rmse values into positive values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00075-1584d72d-611e-414f-8e3a-4d02292eff78",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(\n",
    "    data = gs.cv_results_\n",
    ").filter(\n",
    "    # Extract the split#_test_score and mean_test_score columns\n",
    "    regex = '(split[0-9]+|mean)_test_score'\n",
    ").assign(\n",
    "    # Add the alphas as a column\n",
    "    alpha = alphas\n",
    ")\n",
    "\n",
    "cv_res.update(\n",
    "    # Convert negative rmses to positive\n",
    "    -1 * cv_res.filter(regex = '_test_score')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00076-e022dca0-10cd-478a-9214-e1ad7dacb13e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "This data frame can then be used to plot $\\alpha$ against the mean root mean squared value over the 5 folds, to produce the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00077-b33c04b5-786b-4277-a617-f4815869d7be",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='alpha', y='mean_test_score', data=cv_res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00078-6906ef23-773b-4f86-8897-3520f1a9ed2e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "This plot shows that the value 6.4 is obtained as the minimum of this curve. However, this plot gives us an overly confident view of this choice of this particular value of $\\alpha$. Specifically, if instead of just plotting the mean rmse, we also examine the variablility of that estimate as well as examine the $\\alpha$ vs rmse curve of each fold we see that these estimates are far noisier than they first appeared and we should take the value $\\alpha = 6.4$ with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00079-71351c66-3d4d-49b6-81be-c218262c60d5",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "d = cv_res.melt(\n",
    "    id_vars=('alpha','mean_test_score'),\n",
    "    var_name='fold',\n",
    "    value_name='rmse'\n",
    ")\n",
    "\n",
    "sns.lineplot(x='alpha', y='rmse', color='black', ci = None, data = d)  # Plot the mean rmse +/- the std dev of the rmse.\n",
    "sns.lineplot(x='alpha', y='rmse', hue='fold', data = d) # Plot the curves for each fold\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00080-5d3a5d8b-a473-406a-bd51-c61c7a3d41b6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In the plot above the black line shows the mean rmse across the folds (this is the same curve as shown in the previous plot) and the gray interval indicates + and - 1 standard deviation of the rmses. The other colored cuves shows the rmse curve for each of the different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00081-927b26f5-aacf-44b1-9ba0-feed4d04130d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 13\n",
    "\n",
    "Why do you think that our cross validation results are unstable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00082-7f45e9d2-11d4-47d7-a2fc-03c070d853dc",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00083-7b1263b9-e60e-419a-b511-7e9c54d1cd95",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 2.3 Tuning with RidgeCV\n",
    "\n",
    "Because the process of identifying the value of $\\alpha$ is critical to most uses of Ridge regression, sklearn provides a helpful function called `RidgeCV` which combines `Ridge` with `GridSearchCV`. We import this function from `linear_model` and fit it in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00084-51eac4af-44e8-46a1-909a-ad5bfc756622",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00085-fbe4e1fe-2e2a-485f-a797-5933947cba85",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "r_cv = RidgeCV(\n",
    "    alphas = np.linspace(0.1, 15, num=150), # RidgeCV does not allow alpha=0 for some reason\n",
    "    scoring = \"neg_root_mean_squared_error\"\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00086-efbaace6-ec4d-4b71-a9be-3926b053c09f",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "r_cv = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RidgeCV(\n",
    "        alphas = np.linspace(0.1, 15, num=150), # RidgeCV does not allow alpha=0 for some reason\n",
    "        scoring = \"neg_root_mean_squared_error\"\n",
    "    )\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00087-ec37b424-ec58-48e3-bd98-5ea08043d92c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The resulting model object now has the \"optimal\" value of $\\alpha$ stored in the `alpha_` attribute which we can access directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00088-96679b8c-05a7-4bce-84fe-5e9a044d21f9",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "r_cv.named_steps[\"ridgecv\"].alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00089-45078d3b-54d3-4d87-b268-7bb2255274b9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Additionally, the returned object can be used like any other model object to obtain predictions for the fitted model using this value of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00090-2d449e03-55a6-4f29-ab58-c5ca73213e84",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "model_fit(r_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00091-d9c58e1d-c3c4-4d0c-90e5-8c75331e99dd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 14\n",
    "\n",
    "This model seems to have arrived at a different optimal value for $\\alpha$ compared to using `GridSearchCV` and it also has a different (slightly worse) rmse. Review the documentation for `RidgeCV`. Can you explain this discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00092-5aa642a0-96ef-4706-8f5c-fd1d9d7e506f",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00093-d0f5725b-4ebf-4c66-981b-701b25feca32",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 15\n",
    "\n",
    "Refit the model using the RidgeCV in such a way that you obtain a result similar to what was obtained by `GridSearchCV` (in terms of the optimal $\\alpha$ and validation rmse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00094-04457c80-b8c4-4baa-a90a-9df32ec76586",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00095-7b81d39a-8b3f-47c3-9bb9-589f0d80b6f3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "--- \n",
    "\n",
    "# 3. The Lasso\n",
    "\n",
    "The Lasso is a related modeling approach to Ridge regression, but instead uses an $\\ell_1$ penalty on the coefficients. Mathematically, we can express this model as the solution of the following optimization problem,\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\; \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert_2^2 + \\alpha \\lVert \\boldsymbol{\\beta} \\rVert_1 $$ \n",
    "\n",
    "As with the other models from this worksheet, the `Lasso` model is also provided by the `linear_model` submodule and similarly requires the choice of the tuning parameter `alpha` to determine the weight of the $\\ell_1$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00096-fa6862b8-952b-4c61-9e1a-db0c98096522",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00097-815168b4-4eaa-4825-885f-3611a22f38ea",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "l = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Lasso(alpha=0.15)\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00098-6d80808d-9272-446b-8c47-5afb97ca17cc",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "print(\"lasso rmse:\", model_fit(l, X_test, y_test, plot=True))\n",
    "print(\"lasso coefs:\", get_coefs(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00099-bba3f764-010e-474c-9c55-694d226621a9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 16\n",
    "\n",
    "Adjust the value of `alpha` in the cell above and rerun it. How does the model fit change as alpha changes? How does the validation rmse change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00100-18968c95-1998-4272-809b-1fd0ebd7e317",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00101-1f8a52aa-b1a2-488b-92ac-2ed72a024293",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.1 Lasso $\\beta$s as a function of $\\alpha$\n",
    "\n",
    "As with Ridge regression we can examine the values of $\\beta$ we obtain as tuning parameter $\\alpha$ is adjusted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00102-21fe62f3-69d9-4ca4-b9c8-785c448d9bb8",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.01, 1, num=100)\n",
    "betas = [] # Store coefficients\n",
    "rmses = [] # Store validation rmses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Lasso(alpha=a)\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    # Again ignore the intercept since it isn't included in the penalty\n",
    "    betas.append(get_coefs(m)[1:])  \n",
    "    rmses.append(model_fit(m, X_test, y_test))\n",
    "\n",
    "res = pd.DataFrame(\n",
    "    data = betas,       # Coefficients\n",
    "    columns = X.columns # Coefficient names\n",
    ").assign(\n",
    "    alpha = alphas,     # Add alpahs\n",
    "    rmse = rmses        # Add validation rmses\n",
    ").melt(\n",
    "    id_vars = ('alpha', 'rmse') # Move columns into the rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00103-32a28fcd-a23e-4eb6-ae26-427f0b9a79c2",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='alpha', y='value', hue='variable', data=res).set_title(\"Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00104-c71a1eee-46af-4787-8163-bcc5a444dab4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 17\n",
    "\n",
    "How does the relationship between the $\\beta$s and $\\alpha$ differ from what we saw with the Ridge regression results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00105-d1bb27dd-7510-40f2-bfb2-49fcf3cc2f4d",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00106-7509d70e-6930-4c38-9b65-0ba549931631",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 18\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `lpsa`. *Hint* - think about what the degree of shrinkage towards 0 means in this context. How does this compare to your answer from the ridge regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00107-4f6bc0c0-98a6-4325-89cc-ff9cc5f11c93",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00108-2f0e9a16-7459-4ffc-a3ca-0072d488d2c3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.2 Tuning Lasso\n",
    "\n",
    "We can again use the `GridSearchCV` function to tune our Lasso model and optimize the $\\alpha$ hyperparameter. We avoid using $\\alpha = 0$ as this causes a warning due to the fitting method (coordinate descent) not converging well without regularization (the $\\ell_1$ penalty here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00109-4d51c532-58bc-414c-802f-605dac4c2ccb",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.01, 1, num=100)\n",
    "\n",
    "l_gs = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Lasso()\n",
    "    ),\n",
    "    param_grid={'lasso__alpha': alphas},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00110-7c0a4ed0-810a-40de-a687-b86000034eed",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "print( \"best alpha:\", l_gs.best_params_['lasso__alpha'])\n",
    "print( \"best rmse :\", l_gs.best_score_ * -1)\n",
    "print( \"validation rmse:\", model_fit(l_gs.best_estimator_, X_test, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00111-ff3c6549-422d-4399-a483-11cabdbc1e80",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Worryingly, the chosen alpha is the smallest value provided to our grid search, and hence it has selected the model closest to a linear regression model. We can investigate this further by plotting $\\alpha$ versus the `mean_test_score` values from the `cv_results_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00112-6e28d8a6-269e-48d4-96a3-43fb84254387",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "l_cv_res = pd.DataFrame().assign(\n",
    "    alpha = alphas,\n",
    "    rmse = -1 * l_gs.cv_results_['mean_test_score'],           # mean of the rmse over the folds\n",
    "    rmse_se = l_gs.cv_results_['std_test_score'] / np.sqrt(l_gs.n_splits_), # standard error of the rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00113-b74cbcda-0154-4d5d-8f1b-d78e4131393c",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "        \n",
    "plt.subplot(121)\n",
    "sns.lineplot(x='alpha', y='rmse', data=l_cv_res)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.lineplot(x='alpha', y='rmse', data=l_cv_res).set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00114-36b274fd-4ba6-4518-993a-744d385a6018",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In this case it appears that the model's rmses nearly monotonically increase as $\\alpha$ increases. This indicates that the CV proceedure is exhibiting a preference for the linear regression mode, i.e. a lasso model with no shrinkage. We can check this explicitly by fitting the `LinearRegression` with `GridSearchCV` and comparing the cross validation rmse, this is necessary because our previous modeling did not use any CV to calculate the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00115-dc95af9d-5ad0-4366-9381-8d393c104b0f",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    param_grid = {},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ").fit(\n",
    "    X_train, y_train\n",
    ").best_score_ * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00116-8b7b3414-6737-41b6-8f12-3fdd5e499d81",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In this case the linear regression model does produce a smaller mean rmse than any of the Lasso models. This suggests our choice of model should just be the original linear regression model.\n",
    "\n",
    "*Aside* - However, if we examine these plots closesly, values of $\\alpha$ between 0.01 and 0.1 have very similar mean rmses and there is uncertainty in our estimates of these rmses based on the cross validation folds and the size of our data. One of the suggestions employed by Hastie, et al. in their `glmnet` R package is instead of using the $\\alpha$ with the smallest mean rmse to instead use the largest value of $\\alpha$ that has an error metric (rmse) that is within 1 standard *error* of the minimum value of the error metric (rmse). We can find this value using the `l_cv_res` data frame we previously constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00117-da75b21e-a38d-43f7-9510-5641784e7d0a",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "i = l_cv_res.rmse.idxmin()\n",
    "\n",
    "min_rmse = l_cv_res.rmse[i]       # Smallest rmse\n",
    "min_rmse_se = l_cv_res.rmse_se[i] # Std error of the smallest rmse\n",
    "\n",
    "sub = l_cv_res.rmse <= min_rmse + min_rmse_se # Find rmses w/in 1se of the min + se\n",
    "\n",
    "alpha_1se = l_cv_res.alpha[ sub ].max() # Find the largest alpha\n",
    "\n",
    "alpha_1se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00118-22ec243a-a7e4-493a-96fd-68759ea794c9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "While this approach seems plausible / practical, it should be treated as at best a heuristic as I have not been able to track down any theoretical support for it. Note that we could also employ this strategy even if the minimal $\\alpha$ had not been approximately 0 and it is still likely to be helpful as any increase in $\\alpha$ is likely to reduce the number of coefficients in the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00119-2cc4b242-8bbf-486f-8722-a0f0ef5efbfb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 19\n",
    "\n",
    "If you were to use the $\\alpha_\\text{1se} = 0.26$ which variables would be excluded from the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00120-75700998-6503-48b2-a7b3-f0aae3b5bbd6",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00121-c1ddf4c4-8ccb-4634-ae30-9da03da2dab6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 20\n",
    "\n",
    "If you were to use the $\\alpha_{\\text{1se}} = 0.26$ what would the validation rmse be for this model? How does this compare to the other models we've examined so far? Why might we still prefer this model over the linear regression or Ridge model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00122-648cd704-ea22-4d63-bf80-40f0b0bf856a",
    "deepnote_cell_type": "code",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00123-8b8836d6-23ce-4e0c-b9b8-67e2aee55e2d",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00124-2acb9155-3731-4b46-ad74-c846252f0e60",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 4 Concluding Remarks\n",
    "\n",
    "It is important to notice that throughout the previous two sections we have taken great pains to avoid using our test data to in any way inform our choice of the tuning parameter. Instead, we have always used `KFold` with our training data to obtain the necessary metrics for optimizing the $\\alpha$ hyperparameter. This would also have been possible using the complete data `X` and would have slightly improved our rmse estimates due to the slightly larger sample sizes in the test train splits but it would then mean were repeated using the validation data in the process of determining $\\alpha$ which then puts us at risk for overfitting and therefore having an overly optimistic view of our model's uncertainty.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00125-d4045e58-53c0-4b7f-8aa5-d6248ee21245",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Competing the worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the preceeding exercises. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF and turn it in on gradescope under the `mlp-week05` assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00126-f94a8e2e-6864-478b-9b51-1ca96400115d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10531,
    "execution_start": 1612353465438,
    "output_cleared": true,
    "source_hash": "950a0210",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp-week05.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d29a4ccc-1eda-48dd-8793-bb224e6220f8' target=\"_blank\">\n",
    "<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2f82e0eb-13b2-4937-8c61-4d17582dd74b",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
